<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<!-- Tab icon and title -->
	<title>CSC6052/5051/4100/DDA6307/MDS5110 Natural Language Processing</title>

	<!-- bootstrap template -->
	<link rel="stylesheet" href="./css/bootstrap.min.css">
	<link rel="stylesheet" href="./css/bootstrap-theme.min.css">

	<!-- Google fonts -->
	<link href="./css/font.css" rel="stylesheet" type="text/css">

	<link rel="stylesheet" type="text/css" href="./css/style.css">
	
	<!-- Add CSS for collapsible functionality -->
	<style>
	.collapsible {
		background-color: #f8f8f8;
		color: #333;
		cursor: pointer;
		padding: 15px;
		width: 100%;
		border: 1px solid #ddd;
		text-align: left;
		outline: none;
		font-size: 16px;
		margin: 10px 0;
		font-weight: bold;
		border-radius: 4px;
		transition: all 0.3s ease;
	}
	
	.active, .collapsible:hover {
		background-color: #e7e7e7;
		border-color: #ccc;
	}
	
	.content {
		padding: 20px;
		display: none;
		overflow: hidden;
		background-color: #fff;
		border: 1px solid #ddd;
		border-radius: 4px;
		margin-bottom: 15px;
	}
	</style>

	<!-- jQuery and Bootstrap -->
	<script src="./js/jquery.min.js"></script>
	<script src="./js/bootstrap.min.js"></script>
	
	<!-- Add JavaScript for collapsible functionality -->
	<script>
	document.addEventListener('DOMContentLoaded', function() {
		var coll = document.getElementsByClassName("collapsible");
		for (var i = 0; i < coll.length; i++) {
			coll[i].addEventListener("click", function() {
				this.classList.toggle("active");
				var content = this.nextElementSibling;
				if (content.style.display === "block") {
					content.style.display = "none";
				} else {
					content.style.display = "block";
				}
			});
		}
	});
	</script>
</head>

<body>

	<div id="header">
		<h1>CSC6052/5051/4100/DDA6307/MDS5110 Natural Language Processing </h1>
		<!-- <h1> CIE 6021 Large Language Models</h1> -->
		<div class="text-center">
			<h4>Teaching B 202, Thursday/Friday 10:30AM - 11:50AM, 2025/01/06 - 2025/05/09</h4>
			<h4>Spring 2025</h4>
		</div>
	</div>

	<div class="sechighlight">
		<div class="container sec">
			<div id="coursedesc">
				This course offers a comprehensive study of Natural Language Processing (NLP). We will delve into word representations, the construction of
				language models, the application of deep learning in NLP, and Large Language Models (LLMs). Specifically, we will discuss the architectural
				engineering, data engineering, prompt engineering, training techniques, and efficiency enhancements of LLMs. Students will gain insights into
				the application of NLP and LLMs in various domains, such as text mining, search engines, human-machine interaction, medical-legal consulting,
				low-resource languages, and AI for Science, as well as how to handle issues of data privacy, bias, and ethics. The course will also investigate
				the limitations of NLP and LLMs, such as challenges in alignment. The curriculum includes guest lectures on advanced topics and in-class
				presentations to stimulate practical understanding. This course is ideal for anyone seeking to master the use of NLP and LLMs in their field.
				<br>
				这门课程提供了对自然语言处理（NLP）的全面研究。我们将深入探讨词表示、语言模型的构建、深度学习在NLP中的应用和大型语言模型（LLMs）。特别地，我们将讨论大型语言模型的架构工程、数据工程、提示工程、训练技巧以及效率提升。学生将了解到自然语言处理和大型语言模型在各个领域的应用，如文本挖掘、搜索引擎、人机交互、医疗法律咨询、低资源语言和AI
				for Science，以及如何处理数据隐私、偏见和伦理问题。课程还将研究NLP和LLMs的局限性以及模型对齐问题。该课程包括高级主题的客座讲座和课堂演示。
			</div>
		</div>
	</div>

	<div class="container sec">
		<div class="row">
			<h2><b>Teaching team</b></h2>
			<hr />
			<!-- 教授部分 -->
			<div class="instructor">
				<div class="instructorphoto"><img src="imgs/benyou_new.png"></div>
				<div>Instructor <br> <a href="https://wabyking.github.io/old.html" target="_blank">Benyou Wang</a></div>
			</div>
			
			<div id="coursedesc">
				<br />
				<p>Benyou Wang is an assistant professor in the School of Data Science, The Chinese University of Hong Kong, Shenzhen. He has achieved several
					notable awards, including the Best Paper Nomination Award in SIGIR 2017, Best Explainable NLP Paper in NAACL 2019, Best Paper in NLPCC 2022,
					Marie Curie Fellowship, Huawei Spark Award. His primary focus is on large language models.
					</p>
			</div>
			
			<!-- 助教部分 - 修改这里让它们在同一行 -->
			<div class="row">
				<!-- Leading TA -->
				<div class="col-md-2">
					<div class="instructor">
						<div class="instructorphoto"><img src="imgs/shunian.png"></div>
						<div>Leading TA <br><a href="https://github.com/Shunian-Chen" target="_blank">Shunian Chen</a></div>
					</div>
				</div>
				
				<!-- Other TAs -->
				<div class="col-md-2">
					<div class="instructor">
						<div class="instructorphoto"><img src="https://freedomintelligence.github.io/assets/img/juhaoliang.jpeg"></div>
						<div>TA <br><a href="https://github.com/JuhaoLiang1997" target="_blank">Juhao Liang</a></div>
					</div>
				</div>
				
				<div class="col-md-2">
					<div class="instructor">
						<div class="instructorphoto"><img src="https://freedomintelligence.github.io/assets/img/xidongwang.jpeg"></div>
						<div>TA <br><a href="https://github.com/wangxidong06" target="_blank">Xidong Wang</a></div>
					</div>
				</div>
				
				<div class="col-md-2">
					<div class="instructor">
						<div class="instructorphoto"><img src="imgs/jike_new.png"></div>
						<div>TA <br><a href="https://1ke-ji.github.io/" target="_blank">Ke Ji</a></div>
					</div>
				</div>
				
				<div class="col-md-2">
					<div class="instructor">
						<div class="instructorphoto"><img src="imgs/huangrui.jpg"></div>
						<div>TA <br><a href="https://github.com/OGG4REAL" target="_blank">Rui Huang</a></div>
					</div>
				</div>
				
				<div class="col-md-2">
					<div class="instructor">
						<div class="instructorphoto"><img src="imgs/yuqi.jpg"></div>
						<div>TA <br><a href="https://angela-fei.mystrikingly.com" target="_blank">Yuqi Fei</a></div>
					</div>
				</div>
			</div>
		</div>
	</div>

	<div class="container sec">
		<h2><b>Logistics</b></h2>
		<hr />
		<div id="coursedesc">
			<p>
			<ul>
				<li> <b>Lectures:</b> Teaching B 202, Thursday/Friday 10:30AM - 11:50AM, 2025/01/06 - 2025/05/09 </li>
				<li> <b>Office hours</b> </li>
				<ul>
					<li>Benyou Wang: Thursday 4:00-5:00 PM. Daoyuan Building 504A</li>
					<li>Shunian Chen: Friday 2:00-3:00 PM. Teaching Building D 412 </li>
					<li>Xidong Wang: Wednesday 7:30-8:30 PM. Teaching Building D 412 </li>
					<li>Rui Huang: Tuesday 1:00-2:00 PM. Teaching Building A 521 </li>
					<li>Yuqi Fei: Monday 2:00-3:00 PM. Teaching Building A 517 </li>
				</ul>
				<li> <b>Contact:</b> If you have any question, please reach out to us via email, WeChat group, or post it to BB.</li>
				<!-- <a href="https://join.slack.com/t/csc3160mds6002/shared_invite/zt-1mnxl1sia-Kyqs7RsoMLAvsv8nHSOCHg">Slack</a>. Anyone is welcome to join the slack channel for discussion. -->
			</ul>
			</p>
		</div>
	</div>

	<div class="sechighlight">
		<div class="container sec">
			<h2><b>Course Information</b></h2>
			<hr />
			<div id="coursedesc">
				<p>
					This comprehensive course on Natural Language Processing (NLP) offers a deep dive into the field, providing students with the knowledge and
					skills to understand, design, and implement NLP systems. Starting with an overview of NLP and foundational linguistic concepts, the course
					moves on to word representation and language modeling, essential for understanding text data. It explores how deep learning, from basic
					neural networks to advanced transformer models, has revolutionized NLP and its diverse applications, such as text mining, information
					extraction, and machine translation. The course emphasizes large language models (LLMs), their scaling laws, emergent abilities, training
					strategies, and associated knowledge representation and reasoning. Students will apply their learning in final projects, for example,
					exploring NLP beyond text with multi-modal LLMs, AI for Science, vertical applications and agents. There are guest lectures and in-class
					paper discussions that could learn the cut-edge research. The course also concludes with an examination of NLP's limitations and ethical
					considerations.

					In particular, the topics include:
				<ul>
					<li> Introduction to NLP </li>
					<li> Linguistics and Word Embeddings </li>
					<li> Language Models</li>
					<li> Deep Learning in NLP</li>
					<li> Large Language Models (LLMs) </li>
					<li> Prompt Engineering</li>
					<li> LLM Agents</li>
					<li> Training Large Language Models</li>
					<li> Final Project Introduction and Research Sharing </li>
					<li> Multimodal Learning</li>
					<li> LLM Reasoning and Guest Lecture </li>
					<li> LLM Applications and Guest Lecture </li>
					<li> Project Presentations (Part 1) </li>
					<li> Project Presentations (Part 2) </li>
				</ul>

				</p>
			</div>

			<h3>Prerequisites</h3>
			<ul>

				<li><b>Proficiency in LaTex: </b> All the reports need to be written by using LaTex. A template will be
					provided. If you are not familiar with LaTex, please learn from the <a href="https://www.overleaf.com/learn/latex/Tutorials"
						target="_blank">tutorial</a> in advance.
				</li>
				<li><b>Proficiency in GitHub: </b> All the source codes need to be submitted in GitHub. </li>
				<li><b>Proficiency in Python:</b> All the assignments will be in Python (using Numpy and <a href="https://pytorch.org/"
						target="_blank">PyTorch</a>). </li>
				<li><b>Basic machine learning knowledge: </b> It is possible to take this course without any machine learning knowledge, however, the course
					will be easier if you have foundations of machine learning.
				</li>
				<!-- <li><b>Basic Concepts of probability: </b> It will be easier for you to understand some lectures if you know basics of probability. </li> -->
			</ul>


			<h3>Learning Outcomes</h3>
			<ul>
				<li> <b>Knowledge</b>: a) Students will understand basic concepts and principles of NLP; b) Students could effectively use NLP for daily study,
					work and research; and c) Students will know which tasks NLP are suitable to solve and which are not. </li>
				<li> <b>Skills</b>: a) Students could train NLP models following a complete pipeline and b) Students could call ChatGPT API for daily usage in
					study, work and research. </li>
				<li> <b>Valued/Attitude</b>: a) Students will appreciate the importance of data; b) Students will tend to use data-driven paradigm to solve
					problems; and c) Students will be aware of the limitations and risks of using ChatGPT. </li>
			</ul>



			<!-- <h3>Textbooks</h3>
			<p>
				Recommended Books:
			<ul id="textbooks">
				<li> The course is too cutting-edge to have any textbooks, we might write a white paper during teaching this course. See <a
						href="https://openai.com/blog" target="_blank">OpenAI Blogs</a> for lastest updates.</li>
			</ul>
			</p> -->
			<!-- Collapsible sections for different courses -->
			<h3>Grading Policy</h3>
			<div class="course-sections">
				<!-- CSC4100 Section -->
				<button class="collapsible">CSC4100</button>
				<div class="content">
					<p><b>This grading policy is for students in CSC4100, for students in CSC6052/DDA6307/MDS6002, please refer to the section below.</b></p>
					<h4>Assignments (50%)</h4>
					<ul>
						<li><b>Assignment 1 (15%)</b>: Training word vector.</li>
						<li><b>Assignment 2 (15%)</b>: Using API for testing prompt engineering and LLM agents.</li>
						<li><b>Assignment 3 (20%)</b>: Training NLP model with SFT and RLHF.</li>
						All assignments need a report and code attachment if it has coding. See the relevant evaluation criterion as the final project.
					</ul>

					<h4>Final project (40%)</h4>
					<p> The project could be done by a group but each individual is separately evaluated. You need to write a project report (max 6 pages) for the final
						project.
						Here is the <a href="https://www.overleaf.com/read/fvhykwvngdwz">report template</a>. You are also
						expected to make a project poster presentation. After the final project
						deadline, feel free to make your project open source; we appreciate if you acknowledge this course</p>
					<ul>
						<li><b>Project poster (10%)</b>: Your poster presentation will be rated by other groups and TAs. The average rating will be the final
							credit.</li>
						<ul>
							<li><b> Poster quality (1%)</b>: We all like well-presented posters. </li>
							<li><b> Oral presentation (4%)</b>: Presenters are encouraged to speak clearly and with enthusiasm. </li>
							<li><b> Overall subjective assesment (5%)</b>: Although subjective assesment might be biased, it happens everywhere! </li>
						</ul>
						<li><b>Project report (30%)</b>: The project report will be publicly available after the final poster session. Please let us know if you
							don't wish so.</li>
						<ul>
							<li><b>Technical excitement (7%)</b>: It is encouraged to do something that is either interesting or useful!</li>
							<li><b>Technical soundness (10%)</b>: <!-- ... existing content ... --></li>
							<li><b>Clarity in writing (10%)</b>: <!-- ... existing content ... --></li>
							<li><b>Individual contribution (3%)</b>: <!-- ... existing content ... --></li>
						</ul>
					</ul>
					<li><b>Bonus and penalty</b> Note that the project credit is capped at 40%</li>
					<ul>
						<li><b>TA favorites (2%)</b>: If one of TAs nominates the project as his/her favorite, the involved students would get 1% bonus credit. Each
							TA could nominate one and he or she could reserve his/her nomination. This credict could only be obtained once. </li>
						<li><b>Instructor favorites (1%)</b>: If the instructor nominates the project as his/her favorite, the involved students would get 1% bonus
							credit. Instructor could nominate at most three projects. One could get both TA favorites and Instructor favorites. </li>
						<li><b>Project early-bird bonus (2%)</b>: If you submit the project report by the early submission due date, 2%
							bonus credit will be entitled. </li>
						<li><b>Code reproducibility bonus (1%)</b>: One could obtain this If TAs think they could easily reproduce your results based on the provide
							material.</li>
						<li><b>Ethics concerns (-1%)</b>: If there are any serious ethics concerns by the ethics committee (The instructor and all TAs), the project
							would get 1% penalty. </li>
					</ul>

					<h4>Participation (10%)</h4>
					<p>Here are some ways to earn the participation credit, which is capped at 10%. </p>
					<ul>
						<li><b>Attending guest lectures</b>: In the second half of the course, we have invited speakers. We
							encourage students to attend the guest lectures and participate in Q&A. All students get 2% per
							guest lecture (in total 4%) for either attending in person, or by writing a guest lecture report if
							they attend remotely or watch the recording. </li>
							<li><b>Attending Tutorials</b>: Students are supporsed to attend tutorials, each tutorial account for 1% credit, you are free to be absent for 1 tutorial out of 6, this participation is capped at 5%. </li>
						<li><b>Completing feedback surveys</b>: We will send out two feedback surveys during the semester to evaluate the course and teaching. </li>
						<li><b>User Study</b>: Students are welcone to conduct user study upon their interest; this is not mandatory (thus it does not affect final
							marks).</li>
						<li><b>Course and Teaching Evaluation (CTE)</b>: The school will send requests for CTE to all students.
							The CTE is worth 1% credit.</li>
						<li><b>Volunteer credit (1%)</b>: TAs/instuctor can nominate students for a volunteer credit for those
							who help the poster session organization, or help answer questions from other students (not writing
							assignments). </li>
					</ul>
				</div>

				<!-- CSC6052/DDA6307/MDS6002 Section -->
				<button class="collapsible">CSC6052/DDA6307/MDS6002</button>
				<div class="content">
					<p><b>This grading policy is for students in CSC6052/DDA6307/MDS6002, for students in CSC4100, please refer to the section above.</b></p>
					<h4>Assignments (40%)</h4>
					<ul>
						<li><b>Assignment 1 (10%)</b>: Training word vector.</li>
						<li><b>Assignment 2 (15%)</b>: Using API for testing prompt engineering and LLM agents.</li>
						<li><b>Assignment 3 (15%)</b>: Training NLP model with SFT and RLHF.</li>
						Both assignments need a report and code attachment if it has coding. See the relevant evaluation criterion as the final project.
					</ul>
		<!-- 			<h4>Review of project proposal (10%)</h4>
					<p>We will have a review for project proposals, to assist students better prepare their final projects. A revision is welcome after taking our
						suggestions into consideration.</p> -->
		
					<h4>Final project (55%)</h4>
					<p> The project could be done by a group but each indivisual is separately evaluated. You need to write a project report (max 6 pages) for the final
						project.
						Here is the <a href="https://www.overleaf.com/read/fvhykwvngdwz">report template</a>. You are also
						expected to make a project poster presentation. After the final project
						deadline, feel free to make your project open source; we appreciate if you acknowledge this course</p>
					<ul>
						<li><b>Project poster (10%)</b>: Your poster presentation will be rated by other groups and TAs. The average rating will be the final credit.
						</li>
						<ul>
							<li><b> Poster quality (1%)</b>: We all like well-presented posters. </li>
							<li><b> Oral presentation (4%)</b>: Presenters are encouraged to speak clearly and with enthusiasm. </li>
							<li><b> Overall subjective assesment (5%)</b>: Although subjective assesment might be biased, it happens everywhere! </li>
						</ul>
						<li><b>Project report (45%)</b>: The project report will be publicly available after the final poster
							session. Please let us know if you don't wish so.</li>
						<ul>
							<li><b>Technical excitement (10%)</b>: It is encouraged to do something that is either interesting or useful! Part of them is about the problem to be solved and others are about the solution itself. </li>
							<li><b>Technical soundness (15%)</b>: A) discuss the motivation on
								why you work this project and your algorithm or approach. Even you are reproducing a published
								paper, you should have your own motivation. B) Cite existing related work. C) Present your algorithms or systems for your project.
								Provide
								key information for reviewers to judge whether it is technically correct. D) Provide reasonable evaluation protocol, it should be
								detailed to contexualize your results; E)Report quantitative results and include qualitative evaluation. Analyze and understand your
								system by inspecting key outputs and intermediate results. Discuss
								how it works, when it succeeds and when it fails, and try to interpret why it works and why not. </li>
							<li><b>Clarity in writing (15%)</b>: The report is written in a precise and concise manner so the
								report can be easily understood. </li>
							<li><b>Individual contribution (5%)</b>: This is based on individual contribution, probably on a subjective basis. </li>
						</ul>
						<li><b>Bonus and penalty</b> Note that the project credit is capped at 40%</li>
						<ul>
							<li><b>TA favorites (2%)</b>: If one of TAs nominates the project as his/her favorite, the involved students would get 1% bonus credit. Each
								TA could nominate one and he or she could reserve his/her nomination. This credict could only be obtained once. </li>
							<li><b>Instructor favorites (1%)</b>: If the instructor nominates the project as his/her favorite, the involved students would get 1% bonus
								credit. Instructor could nominate at most three projects. One could get both TA favorites and Instructor favorites. </li>
							<li><b>Project early-bird bonus (2%)</b>: If you submit the project report by the early submission due date, 2%
								bonus credit will be entitled. </li>
							<li><b>Code reproducibility bonus (1%)</b>: One could obtain this If TAs think they could easily reproduce your results based on the provide
								material.</li>
							<li><b>Ethics concerns (-1%)</b>: If there are any serious ethics concerns by the ethics committee (The instructor and all TAs), the project
								would get 1% penalty. </li>
						</ul>
		
					</ul>
					<h4>Participation (5%)</h4>
					<p>Here are some ways to earn the participation credit, which is capped at 5%. </p>
					<ul>
						<li><b>Attending guest lectures</b>: In the second half of the course, we have invited speakers. We
							encourage students to attend the guest lectures and participate in Q&A. All students get 1.5% per
							guest lecture (in total 3%) for either attending in person, or by writing a guest lecture report if
							they attend remotely or watch the recording. </li>
						<li><b>Completing feedback surveys</b>: We will send out two feedback surveys during the semester to evaluate the course and teaching. </li>
						<li><b>User Study</b>: Students are welcone to conduct user study upon their interest; this is not mandatory (thus it does not affect final
							marks).</li>
						<li><b>Course and Teaching Evaluation (CTE)</b>: The school will send requests for CTE to all students.
							The CTE is worth 1% credit.</li>
						<!-- 				<li><b>Slack participation</b>: The top 10 contributors (10 from CSC3100 and 10 from MDS6002) to slack will get 1%; others will get credit in propotion to the 10th contributor. </li>
		 -->
						<li><b>Volunteer credit (1%)</b>: TAs/instuctor can nominate students for a volunteer credit for those
							who help the poster session organization, or help answer questions from other students (not writing
							assignments). </li>
					</ul>
				</div>
			</div>

			<!-- <h3>Grading Policy (CSC6052/DDA6307/MDS6002)</h3>
			 -->

			<h3>Late Policy</h3>
			<p>The penalty is 0.5% off the final course grade for each late day.</p>
		</div>
	</div>


	<div class="container sec">
		<h2><b>Schedule</b></h2>
		<hr />
		<table class="table">
			<tbody>
				<tr class="active">
					<th>Date</th>
					<th>Topics</th>
					<th>Recommended Reading</th>
					<th>Pre-Lecture Questions</th>
					<th>Lecture Note</th>
					<th>Coding</th>
					<th>Events Deadlines</th>
				</tr>

				<tr class="success">
					<td>Jan 6-8 <a style="color: red;">Warmup</a></td>
					<td>Tutorial 0: GitHub, LaTeX, Colab, and ChatGPT API</td>
					<td>
						<a href="https://openai.com/blog/chatgpt">OpenAI's blog</a><br>
						<a href="https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes">LaTeX and Overleaf</a><br>
						<a href="https://colab.research.google.com/">Colab</a><br>
						<a href="https://docs.github.com/en/get-started/quickstart/hello-world/">GitHub</a>
					</td>
					<td></td>
					<td></td>
					<td></td>
					<td></td>
				</tr>

				<tr>
					<td>Jan. 9th</td>
					<td>Lecture 1: Introduction to NLP</td>
					<td>
						<a href="https://huggingface.co/learn/nlp-course/chapter1/1">Hugging Face NLP Course</a><br>
						<a href="https://github.com/mlabonne/llm-course">Course to get into NLP with roadmaps and Colab notebooks.</a><br>
						<a href="https://llm-course.github.io/">LLM-Course</a>
					</td>
					<td>What is NLP?</td>
					<td>[<a href="materials\2025_spring_slides\lecture-1-1-Introduction.pdf">slide</a>]</td>
					<td>[<a href="https://phoenix.llmzoo.com/#/">Phoenix</a>]</td>
					<td></td>
				</tr>

				<tr>
					<td>Jan. 10th</td>
					<td>Lecture 2: Introduction to NLP Cont.</td>
					<td>
						<a href="https://arxiv.org/abs/2108.07258">On the Opportunities and Risks of Foundation Models</a><br>
						<a href="https://arxiv.org/abs/2303.12712">Sparks of Artificial General Intelligence: Early experiments with GPT-4</a>
					</td>
					<td>What is NLP?</td>
					<td>[<a href="materials\2025_spring_slides\lecture-1-2-Introduction.pdf">slide</a>]</td>
					<td></td>
					<td></td>
				</tr>

				<tr>
					<td>Jan. 16th</td>
					<td>Lecture 3: Basics of Linguistics</td>
					<td>
						<a href="https://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf">Universal Stanford Dependencies: A cross-linguistic
							typology</a><br>
						<a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/slides/cs224n-2023-lecture14-insights-linguistics.pdf">Insights
							between NLP and Linguistics </a><br>
						<a href="https://arxiv.org/pdf/1707.07045.pdf">End-to-end Neural Coreference Resolution</a><br>

					</td>
					<td>
       						What is structure of language (string of words)?
					</td>
					<td>[<a href="materials\2025_spring_slides\Lecture 3_ Basics of Linguistics.pdf">slide</a>]</td>
					<td> [<a href="https://github.com/theimpossibleastronaut/awesome-linguistics">Linguistics repo</a>]
					</td>
					<td> </td>
				</tr>


				<tr>
					<td>Jan. 17th</td>
					<td>Lecture 4: Word Representation</td>
					<td>
						<a href="https://arxiv.org/pdf/1301.3781.pdf"> Efficient Estimation of Word Representations in Vector Space (original word2vec paper)
						</a><br>
						<a href="https://aclanthology.org/D15-1036/"> Evaluation methods for unsupervised word embeddings </a><br>
					</td>
					<td>
						How to model language and the inside words?
					</td>
					<td>[<a href="materials\2025_spring_slides\Lecture 4_ Word Representation and Language Modeling.pdf">slide</a>]</td>
					<td>
						[<a href="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/word2vec.ipynb"> word2vec </a>]
					</td>
					<td> <b>Assignment 1 <font color="green">out</font></b><br> </td>
				</tr>

				<tr>
					<td>Feb. 13th</td>
					<td>Lecture 5: Language Modeling</td>
					<td>
						<a href="https://dl.acm.org/doi/10.5555/944919.944966">A Neural Probabilistic Language Model</a><br>
						<a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
					</td>
					<td>
						What is a language model and how does it work in natural language processing?
					</td>
					<td>
						[<a href="materials\2025_spring_slides\Lecture-5-language-model.pdf">slide</a>]
					</td>
					<td>
						[<a href="https://github.com/google-research/bert"> BERT </a>]
					</td>
					<td> </td>
				</tr>

				<tr>
					<td>Feb. 14th</td>
					<td>Lecture 6: Language Modeling Cont.</td>
					<td>
						<a href="https://dl.acm.org/doi/10.5555/944919.944966">A Neural Probabilistic Language Model</a><br>
						<a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
					</td>
					<td>
						What is a language model and how does it work in natural language processing?
					</td>
					<td>
						[<a href="materials\2025_spring_slides\Lecture-6-language-model-cont.pdf">slide</a>]
					</td>
					<td>
					</td>
					<td> </td>
				</tr>

				<tr class="success">
					<td>Feb. 14th</td>
					<td>Tutorial 1: Introduction to Overleaf, GitHub, Python, and Pytorch</td>
					<td>
					</td>
					<td></td>
					<td> </td>
					<td> </td>
					<td> </td>
					<td> </td>
				</tr>


				<tr>
					<td>Feb. 20th</td>
					<td>Lecture 7: Deep Learning in NLP</td>
					<td>
						<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a><br>
						<a href="https://huggingface.co/learn/nlp-course/chapter1/1">HuggingFace's course on Transformers</a><br>
						<a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a><br>
					</td>
					<td>
						How to better compose words semantically as language?
					</td>
					<td>
					</td>
					<td>
						[<a href="https://nn.labml.ai/transformers/index.html">Transformer</a>]
					</td>
					<td></td>
				</tr>

				<tr>
					<td>Feb. 21th</td>
					<td>Lecture 8: Deep Learning in NLP Cont.</td>
					<td>
						<a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0</a><br>
						<a href="https://openreview.net/forum?id=onxoVA9FxMw">On Position Embeddings in BERT</a>
					</td>
					<td>
						How to better compose words semantically as language?
					</td>
					<td>
					</td>
					<td>
					</td>
					<td></td>
				</tr>

				<tr class="success">
					<td>Feb. 21th</td>
					<td>Tutorial 2: Training word embeddings </td>
					<td>
					</td>
					<td></td>
					<td> </td>
					<td>  </td>
					<td> </td>
				</tr>





				<tr>
					<td>Feb. 27th</td>
					<td>Lecture 9: Large Language Models (LLMs)</td>
					<td>
						<a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a><br>
						<a href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a><br>
						<a href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models</a><br>
						<a href="https://openai.com/blog/chatgpt">OpenAI's blog</a>
					</td>
					<td>
						what are LARGE language models and why LARGE?
					</td>
					<td>
					</td>
					<td>
						[<a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing">Fine-tune Llama 2</a>]
					</td>

					<td><b>Assignment 1 <font color="red">due (11:59pm)</font></b>
						<b>Assignment 2 <font color="green">out</font></b><br>
					</td>
				</tr>

				<tr>
					<td>Feb. 28th</td>
					<td>Lecture 10: Large Language Models (LLMs) Cont. </td>
					<td>
						<a href="https://arxiv.org/abs/2412.15115">Qwen2.5 Technical Report</a><br>
						<a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3 Technical Report</a><br>
					</td>
					<td>
						what are LARGE language models and why LARGE?
					</td>
					<td>
					</td>
					<td>
					</td>

					<td> </td>
				</tr>
				<tr>
					<td> Mar. 6th </td>
					<td>Lecture 11: Prompt Engineering </td>
					<td>
						<a href="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api">Best practices for prompt
							engineering with OpenAI API</a><br>
						<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">prompt engineering</a>
					</td>
					<td>
						How to better prompt LLMs?
					</td>
					<td>
					</td>
					<td>
						[<a href="https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb">Prompt_engineer</a>]

					</td>
					<td> </td>
				</tr>
				<tr>
					<td> Mar. 7th </td>
					<td>Lecture 12: Prompt Engineering Cont. </td>
					<td>
					</td>
					<td>
						How to better prompt LLMs?
					</td>
					<td>
					</td>
					<td>

					</td>
					<td> </td>
				</tr>
				<tr class="success">
					<td>Mar. 7th</td>
					<td>Tutorial 3: Prompt Engineering </td>
					<td>
					</td>
					<td></td>
					<td> </td>
					<td> </td>
					<td> </td>
					<td> </td>
				</tr>

				<tr>
					<td> Mar. 13th </td>
					<td>Lecture 13: LLMs as agents </td>
					<td>
						<a href="https://github.com/OpenBMB/ToolBench">ToolBench</a><br>
						<a href="https://arxiv.org/abs/2308.03688">AgentBench</a><br>
						<a href="https://arxiv.org/pdf/2005.11401.pdf">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a><br>
						<a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a><br>
					</td>
					<td>
						How to make LLMs more useful?
					</td>
					<td>
					</td>
					<td>

					</td>
					<td> </td>
				</tr>
				<tr>
					<td> Mar. 14th </td>
					<td>Lecture 14: LLMs as agents Cont. </td>
					<td>
						<a href="https://github.com/X-PLUG/MobileAgent">Mobile Agent</a><br>
						<a href="https://github.com/cline/cline">Cline</a><br>
						<a href="https://github.com/RooVetGit/Roo-Cline">Roo-Cline</a><br>
					</td>
					<td>
						How to make LLMs more useful?
					</td>
					<td>
					</td>
					<td>

					</td>
					<td> </td>
				</tr>
				<tr class="success">
					<td>Mar. 14th</td>
					<td>Tutorial 4: Agent </td>
					<td>
					</td>
					<td></td>
					<td> </td>
					<td> </td>
					<td> </td>
					<td> </td>
				</tr>

				<tr>
					<td> Mar. 20th </td>
					<td>Lecture 15: Training Large Language Models</td>
					<td>
						<a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a><br>
						<a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient
							Sparsity</a><br>
						<a href="https://arxiv.org/abs/2210.17323">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a><br>
					</td>
					<td>
						How to train LLMs from scratch?
					</td>
					<td>
					</td>
					<td>
						[<a href="https://github.com/FreedomIntelligence/HuatuoGPT">HuatuoGPT</a>] <br>
						[<a href="https://github.com/FreedomIntelligence/LLMZoo">LLMZoo</a>]
					</td>
					<td> <b> Assignment 2 <font color="red">due (11:59pm)</font></b>
						<b>Assignment 3 <font color="green">out</font></b><br> </td>
				</tr>

				<tr>
					<td> Mar. 21th </td>
					<td>Lecture 16: Training Large Language Models Cont.</td>
					<td>
						<a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a><br>
						<a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient
							Sparsity</a><br>
						<a href="https://arxiv.org/abs/2210.17323">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a><br>
					</td>
					<td>
						How to train LLMs from scratch?
					</td>
					<td>
					</td>
					<td>
						[<a href="https://github.com/FreedomIntelligence/HuatuoGPT">HuatuoGPT</a>] <br>
						[<a href="https://github.com/FreedomIntelligence/LLMZoo">LLMZoo</a>]
					</td>
					<td> </td>
				</tr>

				<tr class="success">
					<td>Mar. 21th</td>
					<td>Tutorial 5: train your own LLMs </td>
					<td>
					</td>
					<td>Are you ready to train your own LLMs?</td>
					<td> </td>
					<td>[<a href="https://github.com/FreedomIntelligence/LLMZoo">LLMZoo]</a>, [<a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>],
						[<a href="https://github.com/FreedomIntelligence/LLMFactory">LLMFactory</a>] </td>
					<td> </td>
				</tr>

				<tr>
					<td> Mar. 27th </td>
					<td>Lecture 17: Final Projects and Research Sharing</td>
					<td>
					</td>
					<td>
						What are the current research topics in NLP?
					</td>
					<td></td>
					<td></td>
					<td><b>Final Project <font color="green">out</font></b></td>
				</tr>

				<tr>
					<td> Mar. 28th </td>
					<td>Lecture 18: Research Sharing</td>
					<td>
					</td>
					<td>
						What are the current research topics in NLP?
					</td>
					<td> </td>
					<td>
					</td>
					<td>
					</td>
				</tr>

				<tr>
					<td> Apr. 3th </td>
					<td>Lecture 19: NLP and Beyond NLP</td>
					<td>
						<a href="https://lilianweng.github.io/posts/2022-06-09-vlm/"> Blog post: Generalized Visual Language Models </a> <br>
					</td>
					<td>
						Can large models speak, see and perform actions ?
					</td>
					<td>
					</td>
					<td>
						[<a href="https://github.com/NExT-GPT/NExT-GPT">NExT-GPT</a>]
					</td>
					<td>  </td>
				</tr>

				
				<tr class="success">
					<td>Apr. 3th </td>
					<td>Tutorial 6: RLHF: Reinforcement Learning from Human Feedback</td>
					<td>
					</td>
					<td>How to further improve LLMs?</td>
					<td> </td>
					<td> </td>
					<td></td>
					<td></td>
				</tr>

				<tr>
					<td> Apr. 10th </td>
					<td>Lecture 20: NLP and Beyond NLP Cont. </td>
					<td>
					</td>
					<td>
						Can large models speak, see and perform actions ?
					</td>
					<td>
					</td>
					<td>
					</td>
					<td></td>
				</tr>

				<tr>
					<td> Apr. 11th </td>
					<td>Lecture 21: LLM Reasoning</td>
					<td>
						<a href="https://openai.com/o1/">OpenAI's O1</a>
						<a href="https://api-docs.deepseek.com/news/news1120">DeepSeek-R1-Lite-Preview</a>
						<a href="https://qwenlm.github.io/blog/qwq-32b-preview/">Qwen-32B-Preview</a>
						<a href="https://github.com/FreedomIntelligence/HuatuoGPT-o1">HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs</a>
					</td>
					<td>
						How to improve LLMs' reasoning?
					</td>
					<td>

					</td>
					<td>
					</td>
					<td> <b>Assignment 3 <font color="red">due (11:59pm)</font></b> </td>
				</tr>

				<tr>
					<td> Apr. 17th </td>
					<td>Lecture 22:  Guest Lecture </td>
					<td>
					</td>
					<td></td>
					<td></td>
					<td>   
					</td>
					<td>
						<b>Final Project Proposal <font color="red">due (11:59pm)</font></b>
					</td>
				</tr>

				<tr>
					<td> Apr. 18th </td>
					<td>Lecture 23: LLM Applications and Future</td>
					<td>
						<a href="https://arxiv.org/abs/2212.13138">Large Language Models Encode Clinical Knowledge</a>						
						<a href="https://arxiv.org/abs/2202.03629">Survey of Hallucination in Natural Language Generation</a><br>
						<a href="https://openai.com/blog/introducing-superalignment">Superalignment</a><br>
						<a href="https://arxiv.org/abs/2303.10130">GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language	Models</a>
					</td>
					<td>
						What are the future of LLMs?
					</td>
					<td> </td>
					<td>
					</td>
					<td>
						
					</td>
				</tr>

				<tr>
					<td> Apr. 24th </td>
					<td>Lecture 24: LLM Applications and Future Cont. </td>
					<td>
					</td>
					<td>
						What are the future of LLMs?
					</td>
					<td> </td>
					<td>
					</td>
					<td>
						<b>Final Project Poster <font color="red">due (17:00pm)</font></b>
					</td>
				</tr>

				<tr>
					<td> Apr. 25th</td>
					<td>Final Project Presentation: Section 1</td>
					<td>
					</td>
					<td>
						How to solve real-world problems using LLMs
					</td>
					<td> </td>
					<td> </td>
					<td> </td>
				</tr>
			</tr>
			<tr>
				<td> Apr. 27th</td>
				<td>Final Project Presentation: Section 2</td>
				<td>
				</td>
				<td>
					How to solve real-world problems using LLMs
				</td>
				<td> </td>
				<td> </td>
				<td> </td>
			</tr>

			<tr>
				<td> May. 8th</td>
				<td>Final Project Presentation: Section 3</td>
				<td>
				</td>
				<td>
					How to solve real-world problems using LLMs
				</td>
				<td> </td>
				<td> </td>
				<td> </td>
			</tr>

			<tr>
				<td> May. 9th</td>
				<td>Final Project Presentation: Section 4</td>
				<td>
				</td>
				<td>
					How to solve real-world problems using LLMs
				</td>
				<td> </td>
				<td> </td>
				<td> <b>Final Project Report <font color="red">due (11:59pm)</font></b></td>
			</tr>
			</tbody>
		</table>
	</div>

	<div class="">
		<div class="container sec"> </div>
		<hr />
	</div>

	<!-- jQuery and Boostrap -->
	<script src="./js/jquery.min.js"></script>
	<script src="./js/bootstrap.min.js"></script>
	<div class="container sec">
		<h2><b>Acknowledgement</b></h2>
		<p> We borrowed some concepts and the website template from <a href="https://slpcourse.github.io/">[CSC3160/MDS6002] </a> where Prof. Zhizheng Wu is the
			instructor.</p>
		<p> Website github repo is <a href="https://github.com/NLP-Course-CUHKSZ/NLP-course-cuhksz.github.io">[here] </a>.</p>
	</div>
</body>

</html>
